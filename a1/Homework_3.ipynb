{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSE 258, Fall 2019: Homework 3\n",
    "**You’ll probably want to implement your solution by modifying the baseline code provided.**   \n",
    "Files: \n",
    "* http://cseweb.ucsd.edu/classes/fa19/cse258-a/files/assignment1.tar.gz   \n",
    "\n",
    "Kaggle:\n",
    "* https://inclass.kaggle.com/c/cse158258-fa19-read-prediction\n",
    "* (258 only) https://inclass.kaggle.com/c/cse258-fa19-rating-prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tasks (Read prediction)   \n",
    "Since we don’t have access to the test labels, we’ll need to simulate validation/test sets of our own.    \n",
    "So, let’s split the training data (‘train Interactions.csv.gz’) as follows:\n",
    "1. Reviews 1-190,000 for training\n",
    "2. Reviews 190,001-200,000 for validation\n",
    "3. Upload to Kaggle for testing only when you have a good model on the validation set. This will save you time (since Kaggle can take several minutes to return results), and prevent you from exceeding your daily submission limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import scipy\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readGz(path):\n",
    "    for l in gzip.open(path, 'rt'):\n",
    "        yield eval(l)\n",
    "    \n",
    "def readCSV(path):\n",
    "    f = gzip.open(path, 'rt')\n",
    "    header = f.readline()\n",
    "    for l in f:\n",
    "        yield l.strip().split(',')\n",
    "        \n",
    "def accuracy(predictions, labels):\n",
    "    predictions, labels = np.array(predictions), np.array(labels)\n",
    "    return sum(predictions == labels) / len(predictions)\n",
    "\n",
    "def most_popular_percentile(mostPopular, percentile):\n",
    "    return1 = set()\n",
    "    count = 0\n",
    "    for b_count, b in mostPopular:\n",
    "        count += b_count\n",
    "        return1.add(b)\n",
    "        if count > percentile * totalRead: break\n",
    "    return return1\n",
    "\n",
    "# Extra utils for this task\n",
    "def cosine_sim(s1,s2):\n",
    "    numer = len(s1.intersection(s2))\n",
    "    denom = len(s1) * len(s2) + 10**(-8)\n",
    "    return numer / denom \n",
    "    \n",
    "def smallest_cosine(user, book):\n",
    "    users = usersPerBook[book]\n",
    "    b_mark = bookPerUser[user] # Books that user has read\n",
    "    angels = []\n",
    "    for book2 in b_mark:\n",
    "        if book2 == book:\n",
    "            continue\n",
    "        angel = cosine_sim(users, usersPerBook[book2])\n",
    "        angels.append((angel, book2))\n",
    "    angels.sort(reverse=True)\n",
    "    if len(angels) == 0:\n",
    "        return 0\n",
    "    return angels[0][0]\n",
    "\n",
    "def best_pearson_corr(user, book):\n",
    "    # users who rated both books is what we want\n",
    "    avg_rate_book = sum([x[1] for x in reviewsPerBook[book]]) / (len(reviewsPerBook[book]) + 10**(-8))\n",
    "    \n",
    "    reviews_on_book = reviewsPerBook[book]\n",
    "    reviews_on_book_dict = dict(reviews_on_book)\n",
    "    \n",
    "    users_who_rated_book = set([x[0] for x in reviews_on_book])\n",
    "\n",
    "    books_user_have_read = bookPerUser[user]\n",
    "    similarities = []\n",
    "    for book2 in books_user_have_read:\n",
    "        avg_rate_book2 = sum([x[1] for x in reviewsPerBook[book2]])/ (len(reviewsPerBook[book2]) + 10**(-8))\n",
    "        \n",
    "        reviews_on_book2 = reviewsPerBook[book2]\n",
    "        reviews_on_book2_dict = dict(reviews_on_book2)\n",
    "        \n",
    "        users_who_rated_book2 = set([x[0] for x in reviews_on_book2])\n",
    "\n",
    "        users_who_rated_both_books = users_who_rated_book.intersection(users_who_rated_book2)\n",
    "        \n",
    "        # Pearson Correlation happens here:\n",
    "        numer = 0\n",
    "        denom1 = 0\n",
    "        denom2 = 0\n",
    "        for user2 in users_who_rated_both_books:\n",
    "\n",
    "            a = reviews_on_book_dict[user2] - avg_rate_book\n",
    "            b = reviews_on_book2_dict[user2] - avg_rate_book2\n",
    "            numer += a * b\n",
    "            \n",
    "            denom1 += a**2\n",
    "            denom2 += b**2\n",
    "        \n",
    "        denom1 = np.sqrt(denom1)\n",
    "        denom2 = np.sqrt(denom2)\n",
    "        similarities.append((numer/(denom1*denom2 + 10**(-8)), book2))\n",
    "    similarities.sort(reverse=True)\n",
    "    return similarities[0][0]\n",
    "\n",
    "def Jaccard(s1, s2):\n",
    "    numer = len(s1.intersection(s2))\n",
    "    denom = len(s1.union(s2))\n",
    "    return numer / denom\n",
    "\n",
    "def best_jacc(user, book):\n",
    "    users = usersPerBook[book]\n",
    "    b_mark = bookPerUser[user]\n",
    "    similarities = []\n",
    "    for book2 in b_mark:\n",
    "        if book2 == book:\n",
    "            continue\n",
    "        # compute sim between book and book2\n",
    "        sim = Jaccard(users, usersPerBook[book2])\n",
    "        similarities.append(sim)\n",
    "    similarities.sort(reverse=True)\n",
    "    return similarities\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = [line[:2] + [1] for line in readCSV(\"train_Interactions.csv.gz\")] # 1 is the label saying it is read.\n",
    "data = [line for line in readCSV(\"train_Interactions.csv.gz\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extend validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xy_train = data[:190000]\n",
    "Xy_valid = data[190000:]\n",
    "# First get overview of what books each user have read, and what what user a book has been read by.\n",
    "usersPerBook = defaultdict(set)\n",
    "bookPerUser = defaultdict(set)\n",
    "for line in data:\n",
    "    userID, bookID, rating = line\n",
    "    usersPerBook[bookID].add(userID)\n",
    "    bookPerUser[userID].add(bookID)\n",
    "\n",
    "# Randomly ad some negative samples to the validation set\n",
    "negative_samples = []\n",
    "available_books = usersPerBook.keys()\n",
    "for user, book, rating in Xy_valid:\n",
    "    #print(user,book)\n",
    "    random_book = random.choice(list(available_books))\n",
    "    while random_book in bookPerUser[user]:\n",
    "        random_book = random.choice(list(available_books))\n",
    "    new_data = [user, random_book, 0]\n",
    "    negative_samples.append(new_data)\n",
    "Xy_valid += negative_samples # Add the negative data\n",
    "random.shuffle(Xy_valid)\n",
    "\n",
    "Xtrain, ytrain = [d[:2] for d in Xy_train], [int(d[2]) for d in Xy_train]\n",
    "Xvalid, yvalid = [d[:2] for d in Xy_valid], [int(d[2]) for d in Xy_valid]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6980 4000\n",
      "Accuracy: 0.7419\n"
     ]
    }
   ],
   "source": [
    "### Would-read baseline: just rank which books are popular and which are not, and return '1' if a book is among the top-ranked\n",
    "# BASELINE: ACC 0.6576\n",
    "\n",
    "bookCount = defaultdict(int)\n",
    "totalRead = 0\n",
    "for user,book in Xtrain:\n",
    "    bookCount[book] += 1\n",
    "    totalRead += 1\n",
    "mostPopular = [(bookCount[x], x) for x in bookCount]\n",
    "mostPopular.sort(reverse=True)\n",
    "#print(mostPopular)\n",
    "top_10 = most_popular_percentile(mostPopular, 0.20)\n",
    "top_50 = most_popular_percentile(mostPopular, 0.55)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "TP, FP = 0, 0\n",
    "jacc_threshold = 0.012\n",
    "predictions = []\n",
    "for (user, book), rating in zip(Xvalid, yvalid):\n",
    "    # Poppularity\n",
    "    istop10 = book in top_10\n",
    "    istop50 = book in top_50\n",
    "    # Jaccard\n",
    "    jaccard_sims = best_jacc(user, book)\n",
    "    jaccs_over_zero = list(filter(lambda x: x > 0,jaccard_sims))\n",
    "    jacc_avg = sum(jaccs_over_zero) / (len(jaccs_over_zero) + 10**(-8))\n",
    "\n",
    "    pred = 0\n",
    "    if istop10:\n",
    "        pred = 1\n",
    "        # Acc 0.80 here\n",
    "        #print(pred, rating)\n",
    "    elif jacc_avg > jacc_threshold:\n",
    "        # Acc 0.65 here\n",
    "        pred = 1\n",
    "        if pred == rating:\n",
    "            TP += 1\n",
    "        else:\n",
    "            FP += 1\n",
    "    \n",
    "        \n",
    "    \n",
    "    # if pred = 0 -> acc 0.80+\n",
    "\n",
    "        \n",
    "    predictions.append(pred)\n",
    "\n",
    "print(TP, FP)\n",
    "yvalid = list(map(lambda x: int(x>0), yvalid))\n",
    "print(\"Accuracy: {}\".format(accuracy(predictions, yvalid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.2438510862283472\n",
      "1 0.4791666666666667\n",
      "2 0.45698924731182794\n",
      "3 0.4681528662420382\n",
      "4 0.529674003900808\n",
      "5 0.5972299168975069\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This showns that for higher ratings, it is a higher changce for the book to be in the popularpercentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4\n",
    "Improve the above predictor by incorporating both a Jaccard-based threshold and a popularity based threshold. Report the performance on your validation set (1 mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For pearson\n",
    "data = [line for line in readCSV(\"train_Interactions.csv.gz\")]\n",
    "Xy_train_per, Xy_valid_per = data[:190000], data[190000:]\n",
    "Xtrain_per, ytrain_per = [x[:2] for x in Xy_train_per], [int(x[-1]) for x in Xy_train_per]\n",
    "Xvalid_per, yvalid_per = [x[:2] for x in Xy_valid_per], [int(x[-1]) for x in Xy_valid_per]\n",
    "\n",
    "reviewsPerUser = defaultdict(list)\n",
    "reviewsPerBook = defaultdict(list)\n",
    "\n",
    "for user, book, rating in Xy_train_per:\n",
    "    rating = int(rating)\n",
    "    reviewsPerUser[user].append((book, rating))\n",
    "    reviewsPerBook[book].append((user, rating))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each book make a featureset [isPopular, Jaccard] and learn use a Classifier from sklearn to learn the params\n",
    "popularity_th = 0.55\n",
    "popularPercentile = most_popular_percentile(mostPopular, popularity_th) \n",
    "\n",
    "\n",
    "# Calculate features for the classifier to find weights\n",
    "def feature(user, book):\n",
    "    popular = 1 if book in popularPercentile else 0\n",
    "    jac_sim = best_jacc(user, book)\n",
    "    cos = smallest_cosine(user,book)\n",
    "    pearson_corr = best_pearson_corr(user, book)\n",
    "    return [1, jac_sim, pearson_corr]\n",
    "\n",
    "newX = [feature(user, book) for user, book in Xvalid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.93479015,  7.32214529,  1.12944214])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a model that can compute the weights for me\n",
    "model = LogisticRegression(solver=\"lbfgs\", fit_intercept=False, class_weight=\"balanced\")\n",
    "model.fit(newX, yvalid)\n",
    "theta = model.coef_[0]\n",
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload Test Results\n",
    "## Kaggle Username: kristogj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = open(\"predictions_Read.txt\", 'w')\n",
    "for l in open(\"pairs_Read.txt\"):\n",
    "    if l.startswith(\"userID\"):\n",
    "        #header\n",
    "        predictions.write(l)\n",
    "        continue\n",
    "    user, book = l.strip().split('-')\n",
    "    pred = predict(user, book, theta)\n",
    "    predictions.write(user + '-' + book + \",{}\\n\".format(pred))\n",
    "predictions.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (CSE 258 only) Tasks (Rating prediction)\n",
    "\n",
    "Let’s start by building our training/validation sets much as we did for the first task. This time building a validation set is more straightforward: you can simply use part of the data for validation, and do not need to randomly sample non-read users/books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [line for line in readCSV(\"train_Interactions.csv.gz\")]\n",
    "Xy_train, Xy_valid = data[:190000], data[190000:]\n",
    "Xtrain, ytrain = [x[:2] for x in Xy_train], [int(x[-1]) for x in Xy_train]\n",
    "Xvalid, yvalid = [x[:2] for x in Xy_valid], [int(x[-1]) for x in Xy_valid]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 9\n",
    "Fit a predictor of the form\n",
    "\n",
    "$rating(user, item) = \\alpha + \\beta_{user} + \\beta_{item}$\n",
    "\n",
    "\n",
    "by fitting the mean and the two bias terms as described in the lecture notes. Use a regularization\n",
    "parameter of λ = 1. Report the MSE on the validation set (1 mark)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(predictions, labels):\n",
    "    differences = [(x-y)**2 for x,y in zip(predictions,labels)]\n",
    "    return sum(differences) / len(differences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewsPerUser = defaultdict(list)\n",
    "reviewsPerBook = defaultdict(list)\n",
    "\n",
    "for user, book, rating in Xy_train:\n",
    "    rating = int(rating)\n",
    "    reviewsPerUser[user].append(rating)\n",
    "    reviewsPerBook[book].append(rating)\n",
    "\n",
    "ratingMean = sum(ytrain) / len(ytrain)\n",
    "\n",
    "N = len(ytrain)\n",
    "nUsers = len(reviewsPerUser)\n",
    "nBooks = len(reviewsPerBook)\n",
    "users = list(reviewsPerUser.keys())\n",
    "books = list(reviewsPerBook.keys())\n",
    "\n",
    "lamb = 1\n",
    "alpha = ratingMean\n",
    "userBiases = defaultdict(float)\n",
    "bookBiases = defaultdict(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(user, book):\n",
    "    return alpha + userBiases[user] + bookBiases[book]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack(theta):\n",
    "    global alpha\n",
    "    global userBiases\n",
    "    global bookBiases\n",
    "    alpha = theta[0]\n",
    "    userBiases = defaultdict(float)\n",
    "    bookBiases = defaultdict(float)\n",
    "    for user, t in zip(users, theta[1:nUsers+1]):\n",
    "        userBiases[user] = t\n",
    "    for book, t in zip(books, theta[1+nUsers:]):\n",
    "        bookBiases[book] = t\n",
    "\n",
    "def cost(theta, labels, lamb):\n",
    "    unpack(theta)\n",
    "    predictions = [prediction(user, book) for user, book in Xtrain]\n",
    "    cost = MSE(predictions, ytrain)\n",
    "    #print(\"MSE = \" + str(cost))\n",
    "    for u in userBiases:\n",
    "        cost += lamb*userBiases[u]**2\n",
    "    for b in bookBiases:\n",
    "        cost += lamb*bookBiases[b]**2\n",
    "    return cost\n",
    "\n",
    "def derivative(theta, labels, lamb):\n",
    "    unpack(theta)\n",
    "    dalpha = 0\n",
    "    dUserBiases = defaultdict(float)\n",
    "    dBookBiases = defaultdict(float)\n",
    "    for user, book, rating in Xy_train:\n",
    "        rating = int(rating)\n",
    "        pred = prediction(user, book)\n",
    "        diff = pred - rating\n",
    "        dalpha += 2/N*diff\n",
    "        dUserBiases[user] += 2/N*diff\n",
    "        dBookBiases[book] += 2/N*diff\n",
    "    for u in userBiases:\n",
    "        dUserBiases[user] += 2*lamb*userBiases[user]\n",
    "    for i in bookBiases:\n",
    "        dBookBiases[book] += 2*lamb*bookBiases[book]\n",
    "    dtheta = [dalpha] + [dUserBiases[u] for u in users] + [dBookBiases[b] for b in books]\n",
    "    return np.array(dtheta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, f, d = scipy.optimize.fmin_l_bfgs_b(cost, [alpha] + [0.0]*(nUsers+nBooks),\n",
    "                             derivative, args = (ytrain, lamb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 1.4907803977377663\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "for user, book in Xvalid:\n",
    "    predictions.append(prediction(user, book))\n",
    "\n",
    "mse = MSE(predictions, yvalid)\n",
    "print(\"MSE: {}\".format(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 10\n",
    "Report the user and book IDs that have the largest and smallest values of β (1 mark)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Largest user bias: ('u92864068', 0.0004044337058247039) \n",
      " Smallest user bias: ('u11591742', -0.0015810150664586793)\n",
      "\n",
      " Largest book bias: ('b76915592', 0.0008308782940987173) \n",
      " Smallest book bias: ('b57299824', -0.0002723172505094118)\n"
     ]
    }
   ],
   "source": [
    "# Code here\n",
    "userBiases_items = list(userBiases.items())\n",
    "userBiases_items.sort(key=lambda x: x[1], reverse=True)\n",
    "print(\"\\n Largest user bias: {} \\n Smallest user bias: {}\".format(userBiases_items[0], userBiases_items[-1]))\n",
    "\n",
    "bookBiases_items = list(bookBiases.items())\n",
    "bookBiases_items.sort(key=lambda x: x[1], reverse=True)\n",
    "print(\"\\n Largest book bias: {} \\n Smallest book bias: {}\".format(bookBiases_items[0], bookBiases_items[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 11\n",
    "Find a better value of λ using your validation set. Report the value you chose, its MSE, and upload your solution to Kaggle by running it on the test data (1 mark).\n",
    "# Kaggle Username: kristogj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "lamb = 0.00002\n",
    "x, f, d = scipy.optimize.fmin_l_bfgs_b(cost, [alpha] + [0.0]*(nUsers+nBooks),\n",
    "                             derivative, args = (ytrain, lamb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I found the right value for lamb by looping over different values of lamb and observing how the mse was chaning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 1.1742062925437837\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "for user, book in Xvalid:\n",
    "    predictions.append(prediction(user, book))\n",
    "\n",
    "mse = MSE(predictions, yvalid)\n",
    "print(\"MSE: {}\".format(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = open(\"predictions_Rating.txt\", 'w')\n",
    "for l in open(\"pairs_Rating.txt\"):\n",
    "    if l.startswith(\"userID\"):\n",
    "        #header\n",
    "        predictions.write(l)\n",
    "        continue\n",
    "    user, book = l.strip().split('-')\n",
    "    pred = str(prediction(user, book))\n",
    "    predictions.write(user + '-' + book + ',' + pred + '\\n')\n",
    "predictions.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
