{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSE 258, Fall 2019: Homework 3\n",
    "**You’ll probably want to implement your solution by modifying the baseline code provided.**   \n",
    "Files: \n",
    "* http://cseweb.ucsd.edu/classes/fa19/cse258-a/files/assignment1.tar.gz   \n",
    "\n",
    "Kaggle:\n",
    "* https://inclass.kaggle.com/c/cse158258-fa19-read-prediction\n",
    "* (258 only) https://inclass.kaggle.com/c/cse258-fa19-rating-prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tasks (Read prediction)   \n",
    "Since we don’t have access to the test labels, we’ll need to simulate validation/test sets of our own.    \n",
    "So, let’s split the training data (‘train Interactions.csv.gz’) as follows:\n",
    "1. Reviews 1-190,000 for training\n",
    "2. Reviews 190,001-200,000 for validation\n",
    "3. Upload to Kaggle for testing only when you have a good model on the validation set. This will save you time (since Kaggle can take several minutes to return results), and prevent you from exceeding your daily submission limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import scipy\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readGz(path):\n",
    "    for l in gzip.open(path, 'rt'):\n",
    "        yield eval(l)\n",
    "    \n",
    "def readCSV(path):\n",
    "    f = gzip.open(path, 'rt')\n",
    "    header = f.readline()\n",
    "    for l in f:\n",
    "        yield l.strip().split(',')\n",
    "        \n",
    "def accuracy(predictions, labels):\n",
    "    predictions, labels = np.array(predictions), np.array(labels)\n",
    "    return sum(predictions == labels) / len(predictions)\n",
    "\n",
    "def most_popular_percentile(mostPopular, percentile):\n",
    "    return1 = set()\n",
    "    count = 0\n",
    "    for b_count, b in mostPopular:\n",
    "        count += b_count\n",
    "        return1.add(b)\n",
    "        if count > percentile * totalRead: break\n",
    "    return return1\n",
    "\n",
    "def cosine_sim(s1,s2):\n",
    "    numer = len(s1.intersection(s2))\n",
    "    denom = len(s1) * len(s2) + 10**(-8)\n",
    "    return numer / denom \n",
    "    \n",
    "def best_cosine(user, book):\n",
    "    users = usersPerBook[book]\n",
    "    b_mark = bookPerUser[user] # Books that user has read\n",
    "    angels = []\n",
    "    for book2 in b_mark:\n",
    "        if book2 == book:\n",
    "            continue\n",
    "        angel = cosine_sim(users, usersPerBook[book2])\n",
    "        angels.append(angel)\n",
    "    angels.sort(reverse=True)\n",
    "    if len(angels) == 0:\n",
    "        return [0]\n",
    "    return angels\n",
    "\n",
    "def Jaccard(s1, s2):\n",
    "    numer = len(s1.intersection(s2))\n",
    "    denom = len(s1.union(s2))\n",
    "    return numer / denom\n",
    "\n",
    "def best_jacc(user, book):\n",
    "    users = usersPerBook[book]\n",
    "    b_mark = bookPerUser[user]\n",
    "    similarities = []\n",
    "    for book2 in b_mark:\n",
    "        if book2 == book:\n",
    "            continue\n",
    "        # compute sim between book and book2\n",
    "        sim = Jaccard(users, usersPerBook[book2])\n",
    "        similarities.append(sim)\n",
    "    similarities.sort(reverse=True)\n",
    "    return similarities\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [line[:2] + [1] for line in readCSV(\"train_Interactions.csv.gz\")] # 1 is the label saying it is read.\n",
    "#data = [line for line in readCSV(\"train_Interactions.csv.gz\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extend validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xy_train = data[:190000]\n",
    "Xy_valid = data[190000:]\n",
    "# First get overview of what books each user have read, and what what user a book has been read by.\n",
    "usersPerBook = defaultdict(set)\n",
    "bookPerUser = defaultdict(set)\n",
    "for line in Xy_train:\n",
    "    userID, bookID, rating = line\n",
    "    usersPerBook[bookID].add(userID)\n",
    "    bookPerUser[userID].add(bookID)\n",
    "\n",
    "# Randomly ad some negative samples to the validation set\n",
    "negative_samples = []\n",
    "available_books = usersPerBook.keys()\n",
    "for user, book, rating in Xy_valid:\n",
    "    random_book = random.choice(list(available_books))\n",
    "    while random_book in bookPerUser[user]:\n",
    "        random_book = random.choice(list(available_books))\n",
    "    new_data = [user, random_book, 0]\n",
    "    negative_samples.append(new_data)\n",
    "Xy_valid += negative_samples # Add the negative data\n",
    "random.shuffle(Xy_valid)\n",
    "\n",
    "Xtrain, ytrain = [d[:2] for d in Xy_train], [int(d[2]) for d in Xy_train]\n",
    "Xvalid, yvalid = [d[:2] for d in Xy_valid], [int(d[2]) for d in Xy_valid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(sum(yvalid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [line for line in readCSV(\"train_Interactions.csv.gz\")]\n",
    "Xy_train_per, Xy_valid_per = data[:190000], data[190000:]\n",
    "Xtrain_per, ytrain_per = [x[:2] for x in Xy_train_per], [int(x[-1]) for x in Xy_train_per]\n",
    "Xvalid_per, yvalid_per = [x[:2] for x in Xy_valid_per], [int(x[-1]) for x in Xy_valid_per]\n",
    "\n",
    "reviewsPerUser = defaultdict(list)\n",
    "reviewsPerBook = defaultdict(list)\n",
    "\n",
    "for user, book, rating in Xy_train_per:\n",
    "    rating = int(rating)\n",
    "    reviewsPerUser[user].append((book, rating))\n",
    "    reviewsPerBook[book].append((user, rating))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASELINE: ACC 0.6576\n",
    "\n",
    "bookCount = defaultdict(int)\n",
    "totalRead = 0\n",
    "for user,book in Xtrain:\n",
    "    bookCount[book] += 1\n",
    "    totalRead += 1\n",
    "mostPopular = [(bookCount[x], x) for x in bookCount]\n",
    "mostPopular.sort(reverse=True)\n",
    "\n",
    "top_10 = most_popular_percentile(mostPopular, 0.10)\n",
    "top_55 = most_popular_percentile(mostPopular, 0.55)\n",
    "mostPopular.sort()\n",
    "worst30 = most_popular_percentile(mostPopular, 0.30)\n",
    "worst10 = most_popular_percentile(mostPopular, 0.10)\n",
    "\n",
    "def predict(user, book, rating):\n",
    "    global TP, FP, FN, TN\n",
    "    # Poppularity\n",
    "    istop10 = book in top_10\n",
    "    istop55 = book in top_55\n",
    "    isworst30 = book in worst30\n",
    "    isworst10 = book in worst10\n",
    "    \n",
    "    # Jaccard\n",
    "    jaccard_sims = best_jacc(user, book)\n",
    "    jacc_avg = sum(jaccard_sims) / (len(jaccard_sims) + 10**(-8))\n",
    "    \n",
    "    cosine = best_cosine(user, book)\n",
    "    avg_cos = sum(cosine) / len(cosine)\n",
    "\n",
    "    pred = 0\n",
    "    if book in worst30:\n",
    "        if jacc_avg > 0.002:\n",
    "            pred = 1\n",
    "    elif istop10:\n",
    "        pred = 1\n",
    "        if jacc_avg < 0.002:\n",
    "            pred = 0\n",
    "    elif book in top_55:\n",
    "        if jacc_avg > 0.002:\n",
    "            pred = 1\n",
    "    else:\n",
    "        if jacc_avg > 0.002:\n",
    "            pred = 1\n",
    "    return pred\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 3255 6163\n",
      "NUMBER OF ONES PREDICTED 7021 20000\n",
      "Accuracy: 0.67505\n"
     ]
    }
   ],
   "source": [
    "TP, FP, FN, TN = 0, 0, 0, 0\n",
    "avgs = []\n",
    "predictions = []\n",
    "for (user, book), rating in zip(Xvalid, yvalid):\n",
    "    p= predict(user, book, rating) \n",
    "    predictions.append(p)\n",
    "\n",
    "print(TP, FP, FN, TN)\n",
    "print(\"NUMBER OF ONES PREDICTED\",sum(predictions), len(predictions))\n",
    "yvalid = list(map(lambda x: int(x>0), yvalid))\n",
    "print(\"Accuracy: {}\".format(accuracy(predictions, yvalid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0022190594120784627"
      ]
     },
     "execution_count": 473,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(avgs)/len(avgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload Test Results\n",
    "## Kaggle Username: kristogj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "8975 20000\n"
     ]
    }
   ],
   "source": [
    "TP, FP = 0, 0\n",
    "avgs = []\n",
    "predictions = open(\"predictions_Read.txt\", 'w')\n",
    "test_pred = []\n",
    "for l in open(\"pairs_Read.txt\"):\n",
    "    if l.startswith(\"userID\"):\n",
    "        #header\n",
    "        predictions.write(l)\n",
    "        continue\n",
    "    user, book = l.strip().split('-')\n",
    "    pred = predict(user, book)\n",
    "    test_pred.append(pred)\n",
    "    predictions.write(user + '-' + book + \",{}\\n\".format(pred))\n",
    "predictions.close()\n",
    "print(TP, FP)\n",
    "print(sum(test_pred), len(test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.002162103115097757"
      ]
     },
     "execution_count": 475,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(avgs)/len(avgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (CSE 258 only) Tasks (Rating prediction)\n",
    "\n",
    "Let’s start by building our training/validation sets much as we did for the first task. This time building a validation set is more straightforward: you can simply use part of the data for validation, and do not need to randomly sample non-read users/books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [line for line in readCSV(\"train_Interactions.csv.gz\")]\n",
    "Xy_train, Xy_valid = data[:190000], data[190000:]\n",
    "Xtrain, ytrain = [x[:2] for x in Xy_train], [int(x[-1]) for x in Xy_train]\n",
    "Xvalid, yvalid = [x[:2] for x in Xy_valid], [int(x[-1]) for x in Xy_valid]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 9\n",
    "Fit a predictor of the form\n",
    "\n",
    "$rating(user, item) = \\alpha + \\beta_{user} + \\beta_{item}$\n",
    "\n",
    "\n",
    "by fitting the mean and the two bias terms as described in the lecture notes. Use a regularization\n",
    "parameter of λ = 1. Report the MSE on the validation set (1 mark)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(predictions, labels):\n",
    "    differences = [(x-y)**2 for x,y in zip(predictions,labels)]\n",
    "    return sum(differences) / len(differences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewsPerUser = defaultdict(list)\n",
    "reviewsPerBook = defaultdict(list)\n",
    "\n",
    "for user, book, rating in Xy_train:\n",
    "    rating = int(rating)\n",
    "    reviewsPerUser[user].append(rating)\n",
    "    reviewsPerBook[book].append(rating)\n",
    "\n",
    "ratingMean = sum(ytrain) / len(ytrain)\n",
    "\n",
    "N = len(ytrain)\n",
    "nUsers = len(reviewsPerUser)\n",
    "nBooks = len(reviewsPerBook)\n",
    "users = list(reviewsPerUser.keys())\n",
    "books = list(reviewsPerBook.keys())\n",
    "\n",
    "lamb = 1\n",
    "alpha = ratingMean\n",
    "userBiases = defaultdict(float)\n",
    "bookBiases = defaultdict(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(user, book):\n",
    "    return alpha + userBiases[user] + bookBiases[book]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack(theta):\n",
    "    global alpha\n",
    "    global userBiases\n",
    "    global bookBiases\n",
    "    alpha = theta[0]\n",
    "    userBiases = defaultdict(float)\n",
    "    bookBiases = defaultdict(float)\n",
    "    for user, t in zip(users, theta[1:nUsers+1]):\n",
    "        userBiases[user] = t\n",
    "    for book, t in zip(books, theta[1+nUsers:]):\n",
    "        bookBiases[book] = t\n",
    "\n",
    "def cost(theta, labels, lamb):\n",
    "    unpack(theta)\n",
    "    predictions = [prediction(user, book) for user, book in Xtrain]\n",
    "    cost = MSE(predictions, ytrain)\n",
    "    #print(\"MSE = \" + str(cost))\n",
    "    for u in userBiases:\n",
    "        cost += lamb*userBiases[u]**2\n",
    "    for b in bookBiases:\n",
    "        cost += lamb*bookBiases[b]**2\n",
    "    return cost\n",
    "\n",
    "def derivative(theta, labels, lamb):\n",
    "    unpack(theta)\n",
    "    dalpha = 0\n",
    "    dUserBiases = defaultdict(float)\n",
    "    dBookBiases = defaultdict(float)\n",
    "    for user, book, rating in Xy_train:\n",
    "        rating = int(rating)\n",
    "        pred = prediction(user, book)\n",
    "        diff = pred - rating\n",
    "        dalpha += 2/N*diff\n",
    "        dUserBiases[user] += 2/N*diff\n",
    "        dBookBiases[book] += 2/N*diff\n",
    "    for u in userBiases:\n",
    "        dUserBiases[user] += 2*lamb*userBiases[user]\n",
    "    for i in bookBiases:\n",
    "        dBookBiases[book] += 2*lamb*bookBiases[book]\n",
    "    dtheta = [dalpha] + [dUserBiases[u] for u in users] + [dBookBiases[b] for b in books]\n",
    "    return np.array(dtheta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, f, d = scipy.optimize.fmin_l_bfgs_b(cost, [alpha] + [0.0]*(nUsers+nBooks),\n",
    "                             derivative, args = (ytrain, lamb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 1.4907803977377663\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "for user, book in Xvalid:\n",
    "    predictions.append(prediction(user, book))\n",
    "\n",
    "mse = MSE(predictions, yvalid)\n",
    "print(\"MSE: {}\".format(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 10\n",
    "Report the user and book IDs that have the largest and smallest values of β (1 mark)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Largest user bias: ('u92864068', 0.0004044337058247039) \n",
      " Smallest user bias: ('u11591742', -0.0015810150664586793)\n",
      "\n",
      " Largest book bias: ('b76915592', 0.0008308782940987173) \n",
      " Smallest book bias: ('b57299824', -0.0002723172505094118)\n"
     ]
    }
   ],
   "source": [
    "# Code here\n",
    "userBiases_items = list(userBiases.items())\n",
    "userBiases_items.sort(key=lambda x: x[1], reverse=True)\n",
    "print(\"\\n Largest user bias: {} \\n Smallest user bias: {}\".format(userBiases_items[0], userBiases_items[-1]))\n",
    "\n",
    "bookBiases_items = list(bookBiases.items())\n",
    "bookBiases_items.sort(key=lambda x: x[1], reverse=True)\n",
    "print(\"\\n Largest book bias: {} \\n Smallest book bias: {}\".format(bookBiases_items[0], bookBiases_items[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 11\n",
    "Find a better value of λ using your validation set. Report the value you chose, its MSE, and upload your solution to Kaggle by running it on the test data (1 mark).\n",
    "# Kaggle Username: kristogj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "lamb = 0.00002\n",
    "x, f, d = scipy.optimize.fmin_l_bfgs_b(cost, [alpha] + [0.0]*(nUsers+nBooks),\n",
    "                             derivative, args = (ytrain, lamb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I found the right value for lamb by looping over different values of lamb and observing how the mse was chaning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 1.1742062925437837\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "for user, book in Xvalid:\n",
    "    predictions.append(prediction(user, book))\n",
    "\n",
    "mse = MSE(predictions, yvalid)\n",
    "print(\"MSE: {}\".format(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = open(\"predictions_Rating.txt\", 'w')\n",
    "for l in open(\"pairs_Rating.txt\"):\n",
    "    if l.startswith(\"userID\"):\n",
    "        #header\n",
    "        predictions.write(l)\n",
    "        continue\n",
    "    user, book = l.strip().split('-')\n",
    "    pred = str(prediction(user, book))\n",
    "    predictions.write(user + '-' + book + ',' + pred + '\\n')\n",
    "predictions.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
