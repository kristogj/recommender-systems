{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSE 258, Fall 2019: Homework 3\n",
    "**You’ll probably want to implement your solution by modifying the baseline code provided.**   \n",
    "Files: \n",
    "* http://cseweb.ucsd.edu/classes/fa19/cse258-a/files/assignment1.tar.gz   \n",
    "\n",
    "Kaggle:\n",
    "* https://inclass.kaggle.com/c/cse158258-fa19-read-prediction\n",
    "* (258 only) https://inclass.kaggle.com/c/cse258-fa19-rating-prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tasks (Read prediction)   \n",
    "Since we don’t have access to the test labels, we’ll need to simulate validation/test sets of our own.    \n",
    "So, let’s split the training data (‘train Interactions.csv.gz’) as follows:\n",
    "1. Reviews 1-190,000 for training\n",
    "2. Reviews 190,001-200,000 for validation\n",
    "3. Upload to Kaggle for testing only when you have a good model on the validation set. This will save you time (since Kaggle can take several minutes to return results), and prevent you from exceeding your daily submission limit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import scipy\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Help Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readGz(path):\n",
    "    for l in gzip.open(path, 'rt'):\n",
    "        yield eval(l)\n",
    "    \n",
    "def readCSV(path):\n",
    "    f = gzip.open(path, 'rt')\n",
    "    header = f.readline()\n",
    "    for l in f:\n",
    "        yield l.strip().split(',')\n",
    "        \n",
    "def accuracy(predictions, labels):\n",
    "    predictions, labels = np.array(predictions), np.array(labels)\n",
    "    return sum(predictions == labels) / len(predictions)\n",
    "\n",
    "def most_popular_percentile(mostPopular, percentile):\n",
    "    return1 = set()\n",
    "    count = 0\n",
    "    for b_count, b in mostPopular:\n",
    "        count += b_count\n",
    "        return1.add(b)\n",
    "        if count > percentile * totalRead: break\n",
    "    return return1\n",
    "\n",
    "def cosine_sim(s1,s2):\n",
    "    numer = len(s1.intersection(s2))\n",
    "    denom = len(s1) * len(s2) + 10**(-8)\n",
    "    return numer / denom \n",
    "    \n",
    "def best_cosine(user, book):\n",
    "    users = usersPerBook[book]\n",
    "    b_mark = bookPerUser[user] # Books that user has read\n",
    "    angels = []\n",
    "    for book2 in b_mark:\n",
    "        if book2 == book:\n",
    "            continue\n",
    "        angel = cosine_sim(users, usersPerBook[book2])\n",
    "        angels.append(angel)\n",
    "    angels.sort(reverse=True)\n",
    "    if len(angels) == 0:\n",
    "        return [0]\n",
    "    return angels\n",
    "\n",
    "def Jaccard(s1, s2):\n",
    "    numer = len(s1.intersection(s2))\n",
    "    denom = len(s1.union(s2))\n",
    "    return numer / denom\n",
    "\n",
    "def best_jacc(user, book):\n",
    "    users = usersPerBook[book]\n",
    "    b_mark = bookPerUser[user]\n",
    "    similarities = []\n",
    "    for book2 in b_mark:\n",
    "        if book2 == book:\n",
    "            continue\n",
    "        # compute sim between book and book2\n",
    "        sim = Jaccard(users, usersPerBook[book2])\n",
    "        similarities.append(sim)\n",
    "    similarities.sort(reverse=True)\n",
    "    return similarities\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [line[:2] + [1] for line in readCSV(\"train_Interactions.csv.gz\")] # 1 is the label saying it is read.\n",
    "#data = [line for line in readCSV(\"train_Interactions.csv.gz\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extend validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xy_train = data[:190000]\n",
    "Xy_valid = data[190000:]\n",
    "# First get overview of what books each user have read, and what what user a book has been read by.\n",
    "usersPerBook = defaultdict(set)\n",
    "bookPerUser = defaultdict(set)\n",
    "for line in Xy_train:\n",
    "    userID, bookID, rating = line\n",
    "    usersPerBook[bookID].add(userID)\n",
    "    bookPerUser[userID].add(bookID)\n",
    "\n",
    "# Randomly ad some negative samples to the validation set\n",
    "negative_samples = []\n",
    "available_books = usersPerBook.keys()\n",
    "for user, book, rating in Xy_valid:\n",
    "    random_book = random.choice(list(available_books))\n",
    "    while random_book in bookPerUser[user]:\n",
    "        random_book = random.choice(list(available_books))\n",
    "    new_data = [user, random_book, 0]\n",
    "    negative_samples.append(new_data)\n",
    "Xy_valid += negative_samples # Add the negative data\n",
    "random.shuffle(Xy_valid)\n",
    "\n",
    "Xtrain, ytrain = [d[:2] for d in Xy_train], [int(d[2]) for d in Xy_train]\n",
    "Xvalid, yvalid = [d[:2] for d in Xy_valid], [int(d[2]) for d in Xy_valid]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASELINE: ACC 0.6576\n",
    "\n",
    "bookCount = defaultdict(int)\n",
    "totalRead = 0\n",
    "for user,book in Xtrain:\n",
    "    bookCount[book] += 1\n",
    "    totalRead += 1\n",
    "mostPopular = [(bookCount[x], x) for x in bookCount]\n",
    "mostPopular.sort(reverse=True)\n",
    "\n",
    "top_6 = most_popular_percentile(mostPopular, 0.06)\n",
    "top_10 = most_popular_percentile(mostPopular, 0.10)\n",
    "top_30 = most_popular_percentile(mostPopular, 0.30)\n",
    "mostPopular.sort()\n",
    "worst30 = most_popular_percentile(mostPopular, 0.30)\n",
    "worst10 = most_popular_percentile(mostPopular, 0.10)\n",
    "\n",
    "def predict(user, book):\n",
    "    global TP, FP, FN, TN\n",
    "    # Poppularity\n",
    "    istop6 = book in top_6\n",
    "    istop10 = book in top_10\n",
    "    istop30 = book in top_30\n",
    "    isworst30 = book in worst30\n",
    "    isworst10 = book in worst10\n",
    "    \n",
    "    \n",
    "    # Jaccard\n",
    "    jaccard_sims = best_jacc(user, book)\n",
    "    jacc_avg = sum(jaccard_sims) / (len(jaccard_sims) + 10**(-8))\n",
    "    \n",
    "    pred = 0\n",
    "    if isworst10:\n",
    "        pred = 0\n",
    "    elif isworst30:\n",
    "        if jacc_avg > 0.0025:\n",
    "            pred = 1\n",
    "    elif istop10:\n",
    "        pred = 1\n",
    "        if jacc_avg < 0.0014:\n",
    "            pred = 0\n",
    "    else:\n",
    "        \n",
    "        if jacc_avg > 0.0016:\n",
    "            pred = 1\n",
    "    return pred        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBER OF ONES PREDICTED 9067 20000\n",
      "Accuracy: 0.6871\n"
     ]
    }
   ],
   "source": [
    "TP, FP, FN, TN = 0, 0, 0, 0\n",
    "avgs = []\n",
    "predictions = []\n",
    "for (user, book), rating in zip(Xvalid, yvalid):\n",
    "    p= predict(user, book) \n",
    "    predictions.append(p)\n",
    "\n",
    "\n",
    "print(\"NUMBER OF ONES PREDICTED\",sum(predictions), len(predictions))\n",
    "yvalid = list(map(lambda x: int(x>0), yvalid))\n",
    "print(\"Accuracy: {}\".format(accuracy(predictions, yvalid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload Test Results\n",
    "## Kaggle Username: kristogj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "9009 20000\n"
     ]
    }
   ],
   "source": [
    "TP, FP = 0, 0\n",
    "avgs = []\n",
    "predictions = open(\"predictions_Read.txt\", 'w')\n",
    "test_pred = []\n",
    "for l in open(\"pairs_Read.txt\"):\n",
    "    if l.startswith(\"userID\"):\n",
    "        #header\n",
    "        predictions.write(l)\n",
    "        continue\n",
    "    user, book = l.strip().split('-')\n",
    "    pred = predict(user, book)\n",
    "    test_pred.append(pred)\n",
    "    predictions.write(user + '-' + book + \",{}\\n\".format(pred))\n",
    "predictions.close()\n",
    "print(TP, FP)\n",
    "print(sum(test_pred), len(test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (CSE 258 only) Tasks (Rating prediction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [line for line in readCSV(\"train_Interactions.csv.gz\")]\n",
    "Xy_train, Xy_valid = data[:190000], data[190000:]\n",
    "Xtrain, ytrain = [x[:2] for x in Xy_train], [int(x[-1]) for x in Xy_train]\n",
    "Xvalid, yvalid = [x[:2] for x in Xy_valid], [int(x[-1]) for x in Xy_valid]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Help Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(predictions, labels):\n",
    "    differences = [(x-y)**2 for x,y in zip(predictions,labels)]\n",
    "    return sum(differences) / len(differences)\n",
    "\n",
    "def prediction(user, book):\n",
    "    return alpha + userBiases[user] + bookBiases[book]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert data into fitting datastructures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reviews\n",
    "reviewsPerUser = defaultdict(list)\n",
    "reviewsPerBook = defaultdict(list)\n",
    "\n",
    "# Ids\n",
    "usersPerBook = defaultdict(set)\n",
    "bookPerUser = defaultdict(set)\n",
    "\n",
    "for user, book, rating in Xy_train:\n",
    "    rating = int(rating)\n",
    "    reviewsPerUser[user].append((book,rating))\n",
    "    reviewsPerBook[book].append((user,rating))\n",
    "    usersPerBook[book].add(user)\n",
    "    bookPerUser[user].add(book)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model\n",
    "Fit a predictor of the form\n",
    "\n",
    "$rating(user, item) = \\alpha + \\beta_{user} + \\beta_{item}$\n",
    "\n",
    "\n",
    "by fitting the mean and the two bias terms as described in the lecture notes. Use a regularization\n",
    "parameter of λ = 1. Report the MSE on the validation set (1 mark)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.897121052631579\n",
      "3.8712060830448047\n",
      "3.852192167723789\n",
      "3.8384933096740084\n",
      "3.828690371709317\n",
      "3.821711945691049\n",
      "3.8167643390321606\n",
      "3.8132676916326615\n",
      "3.8108026739174696\n",
      "3.8090683669341434\n",
      "3.807850089901746\n",
      "3.806995379716472\n",
      "3.806396342363318\n",
      "3.8059768365503395\n",
      "3.8056832472129227\n",
      "3.8054778871324006\n",
      "3.8053343020114974\n"
     ]
    }
   ],
   "source": [
    "lamb = 2.6\n",
    "userBiases = defaultdict(float)\n",
    "bookBiases = defaultdict(float)\n",
    "N = len(Xtrain)\n",
    "\n",
    "def MSE(predictions, labels):\n",
    "    differences = [(x-y)**2 for x,y in zip(predictions,labels)]\n",
    "    return sum(differences) / len(differences)\n",
    "\n",
    "\n",
    "last_alpha = 1\n",
    "alpha = 0\n",
    "# While a threshold\n",
    "counter = 0\n",
    "while abs(alpha - last_alpha)  > 0.00001:\n",
    "    last_alpha = alpha\n",
    "    \n",
    "    # Calculate new alpha\n",
    "    alpha = 0\n",
    "    for (user, book), rating in zip(Xtrain, ytrain):\n",
    "        rating = int(rating)\n",
    "        alpha += (rating - (userBiases[user] + bookBiases[book]))\n",
    "    alpha /= N\n",
    "    \n",
    "    # Reset userBiases\n",
    "    for user in bookPerUser.keys():\n",
    "            userBiases[user] = 0\n",
    "            \n",
    "    # Calculate new userBiases\n",
    "    for (user, book), rating in zip(Xtrain, ytrain):\n",
    "        rating = int(rating)\n",
    "        n = lamb + len(reviewsPerUser[user])\n",
    "        userBiases[user] += (rating - (alpha + bookBiases[book])) / n\n",
    "    \n",
    "    # Reset bookBiases\n",
    "    for book in usersPerBook.keys():\n",
    "            bookBiases[book] = 0\n",
    "    # Calculate new bookBiases\n",
    "    for (user, book), rating in zip(Xtrain, ytrain):\n",
    "        rating = int(rating)\n",
    "        n = lamb + len(reviewsPerBook[book])\n",
    "        bookBiases[book] += (rating - (alpha + userBiases[user]))/ n\n",
    "    \n",
    "    if counter % 10 == 0:  \n",
    "        print(alpha)\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 1.1159070411813894\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "for user, book in Xvalid:\n",
    "    predictions.append(prediction(user, book))\n",
    "\n",
    "mse = MSE(predictions, yvalid)\n",
    "print(\"MSE: {}\".format(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 10\n",
    "Report the user and book IDs that have the largest and smallest values of β (1 mark)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Largest user bias: ('u76356300', 1.1110622403117367) \n",
      " Smallest user bias: ('u79141508', -3.288154534217488)\n",
      "\n",
      " Largest book bias: ('b19925500', 0.9630623763400692) \n",
      " Smallest book bias: ('b84091840', -1.4275795293084055)\n"
     ]
    }
   ],
   "source": [
    "# Code here\n",
    "userBiases_items = list(userBiases.items())\n",
    "userBiases_items.sort(key=lambda x: x[1], reverse=True)\n",
    "print(\"\\n Largest user bias: {} \\n Smallest user bias: {}\".format(userBiases_items[0], userBiases_items[-1]))\n",
    "\n",
    "bookBiases_items = list(bookBiases.items())\n",
    "bookBiases_items.sort(key=lambda x: x[1], reverse=True)\n",
    "print(\"\\n Largest book bias: {} \\n Smallest book bias: {}\".format(bookBiases_items[0], bookBiases_items[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 11\n",
    "Find a better value of λ using your validation set. Report the value you chose, its MSE, and upload your solution to Kaggle by running it on the test data (1 mark).\n",
    "# Kaggle Username: kristogj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I found the right value for lamb by looping over different values of lamb and observing how the mse was chaning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = open(\"predictions_Rating.txt\", 'w')\n",
    "for l in open(\"pairs_Rating.txt\"):\n",
    "    if l.startswith(\"userID\"):\n",
    "        #header\n",
    "        predictions.write(l)\n",
    "        continue\n",
    "    user, book = l.strip().split('-')\n",
    "    pred = str(prediction(user, book))\n",
    "    predictions.write(user + '-' + book + ',' + pred + '\\n')\n",
    "predictions.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
