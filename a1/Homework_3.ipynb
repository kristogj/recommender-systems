{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSE 258, Fall 2019: Homework 3\n",
    "**You’ll probably want to implement your solution by modifying the baseline code provided.**   \n",
    "Files: \n",
    "* http://cseweb.ucsd.edu/classes/fa19/cse258-a/files/assignment1.tar.gz   \n",
    "\n",
    "Kaggle:\n",
    "* https://inclass.kaggle.com/c/cse158258-fa19-read-prediction\n",
    "* (258 only) https://inclass.kaggle.com/c/cse258-fa19-rating-prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tasks (Read prediction)   \n",
    "Since we don’t have access to the test labels, we’ll need to simulate validation/test sets of our own.    \n",
    "So, let’s split the training data (‘train Interactions.csv.gz’) as follows:\n",
    "1. Reviews 1-190,000 for training\n",
    "2. Reviews 190,001-200,000 for validation\n",
    "3. Upload to Kaggle for testing only when you have a good model on the validation set. This will save you time (since Kaggle can take several minutes to return results), and prevent you from exceeding your daily submission limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import scipy\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readGz(path):\n",
    "    for l in gzip.open(path, 'rt'):\n",
    "        yield eval(l)\n",
    "    \n",
    "def readCSV(path):\n",
    "    f = gzip.open(path, 'rt')\n",
    "    header = f.readline()\n",
    "    for l in f:\n",
    "        yield l.strip().split(',')\n",
    "        \n",
    "def accuracy(predictions, labels):\n",
    "    predictions, labels = np.array(predictions), np.array(labels)\n",
    "    return sum(predictions == labels) / len(predictions)\n",
    "\n",
    "def most_popular_percentile(mostPopular, percentile):\n",
    "    return1 = set()\n",
    "    count = 0\n",
    "    for b_count, b in mostPopular:\n",
    "        count += b_count\n",
    "        return1.add(b)\n",
    "        if count > percentile * totalRead: break\n",
    "    return return1\n",
    "\n",
    "def cosine_sim(s1,s2):\n",
    "    numer = len(s1.intersection(s2))\n",
    "    denom = len(s1) * len(s2) + 10**(-8)\n",
    "    return numer / denom \n",
    "    \n",
    "def best_cosine(user, book):\n",
    "    users = usersPerBook[book]\n",
    "    b_mark = bookPerUser[user] # Books that user has read\n",
    "    angels = []\n",
    "    for book2 in b_mark:\n",
    "        if book2 == book:\n",
    "            continue\n",
    "        angel = cosine_sim(users, usersPerBook[book2])\n",
    "        angels.append(angel)\n",
    "    angels.sort(reverse=True)\n",
    "    if len(angels) == 0:\n",
    "        return [0]\n",
    "    return angels\n",
    "\n",
    "def Jaccard(s1, s2):\n",
    "    numer = len(s1.intersection(s2))\n",
    "    denom = len(s1.union(s2))\n",
    "    return numer / denom\n",
    "\n",
    "def best_jacc(user, book):\n",
    "    users = usersPerBook[book]\n",
    "    b_mark = bookPerUser[user]\n",
    "    similarities = []\n",
    "    for book2 in b_mark:\n",
    "        if book2 == book:\n",
    "            continue\n",
    "        # compute sim between book and book2\n",
    "        sim = Jaccard(users, usersPerBook[book2])\n",
    "        similarities.append(sim)\n",
    "    similarities.sort(reverse=True)\n",
    "    return similarities\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [line[:2] + [1] for line in readCSV(\"train_Interactions.csv.gz\")] # 1 is the label saying it is read.\n",
    "#data = [line for line in readCSV(\"train_Interactions.csv.gz\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extend validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xy_train = data[:190000]\n",
    "Xy_valid = data[190000:]\n",
    "# First get overview of what books each user have read, and what what user a book has been read by.\n",
    "usersPerBook = defaultdict(set)\n",
    "bookPerUser = defaultdict(set)\n",
    "for line in Xy_train:\n",
    "    userID, bookID, rating = line\n",
    "    usersPerBook[bookID].add(userID)\n",
    "    bookPerUser[userID].add(bookID)\n",
    "\n",
    "# Randomly ad some negative samples to the validation set\n",
    "negative_samples = []\n",
    "available_books = usersPerBook.keys()\n",
    "for user, book, rating in Xy_valid:\n",
    "    random_book = random.choice(list(available_books))\n",
    "    while random_book in bookPerUser[user]:\n",
    "        random_book = random.choice(list(available_books))\n",
    "    new_data = [user, random_book, 0]\n",
    "    negative_samples.append(new_data)\n",
    "Xy_valid += negative_samples # Add the negative data\n",
    "random.shuffle(Xy_valid)\n",
    "\n",
    "Xtrain, ytrain = [d[:2] for d in Xy_train], [int(d[2]) for d in Xy_train]\n",
    "Xvalid, yvalid = [d[:2] for d in Xy_valid], [int(d[2]) for d in Xy_valid]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASELINE: ACC 0.6576\n",
    "\n",
    "bookCount = defaultdict(int)\n",
    "totalRead = 0\n",
    "for user,book in Xtrain:\n",
    "    bookCount[book] += 1\n",
    "    totalRead += 1\n",
    "mostPopular = [(bookCount[x], x) for x in bookCount]\n",
    "mostPopular.sort(reverse=True)\n",
    "\n",
    "top_6 = most_popular_percentile(mostPopular, 0.06)\n",
    "top_10 = most_popular_percentile(mostPopular, 0.10)\n",
    "top_30 = most_popular_percentile(mostPopular, 0.30)\n",
    "mostPopular.sort()\n",
    "worst30 = most_popular_percentile(mostPopular, 0.30)\n",
    "worst10 = most_popular_percentile(mostPopular, 0.10)\n",
    "\n",
    "def predict(user, book):\n",
    "    global TP, FP, FN, TN\n",
    "    # Poppularity\n",
    "    istop6 = book in top_6\n",
    "    istop10 = book in top_10\n",
    "    istop30 = book in top_30\n",
    "    isworst30 = book in worst30\n",
    "    isworst10 = book in worst10\n",
    "    \n",
    "    \n",
    "    # Jaccard\n",
    "    jaccard_sims = best_jacc(user, book)\n",
    "    jacc_avg = sum(jaccard_sims) / (len(jaccard_sims) + 10**(-8))\n",
    "    \n",
    "    pred = 0\n",
    "    if isworst10:\n",
    "        pred = 0\n",
    "    elif isworst30:\n",
    "        if jacc_avg > 0.0025:\n",
    "            pred = 1\n",
    "    elif istop10:\n",
    "        pred = 1\n",
    "        if jacc_avg < 0.0014:\n",
    "            pred = 0\n",
    "    else:\n",
    "        \n",
    "        if jacc_avg > 0.0016:\n",
    "            pred = 1\n",
    "    return pred        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBER OF ONES PREDICTED 9067 20000\n",
      "Accuracy: 0.6871\n"
     ]
    }
   ],
   "source": [
    "TP, FP, FN, TN = 0, 0, 0, 0\n",
    "avgs = []\n",
    "predictions = []\n",
    "for (user, book), rating in zip(Xvalid, yvalid):\n",
    "    p= predict(user, book) \n",
    "    predictions.append(p)\n",
    "\n",
    "\n",
    "print(\"NUMBER OF ONES PREDICTED\",sum(predictions), len(predictions))\n",
    "yvalid = list(map(lambda x: int(x>0), yvalid))\n",
    "print(\"Accuracy: {}\".format(accuracy(predictions, yvalid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload Test Results\n",
    "## Kaggle Username: kristogj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "9009 20000\n"
     ]
    }
   ],
   "source": [
    "TP, FP = 0, 0\n",
    "avgs = []\n",
    "predictions = open(\"predictions_Read.txt\", 'w')\n",
    "test_pred = []\n",
    "for l in open(\"pairs_Read.txt\"):\n",
    "    if l.startswith(\"userID\"):\n",
    "        #header\n",
    "        predictions.write(l)\n",
    "        continue\n",
    "    user, book = l.strip().split('-')\n",
    "    pred = predict(user, book)\n",
    "    test_pred.append(pred)\n",
    "    predictions.write(user + '-' + book + \",{}\\n\".format(pred))\n",
    "predictions.close()\n",
    "print(TP, FP)\n",
    "print(sum(test_pred), len(test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.002162103115097757"
      ]
     },
     "execution_count": 475,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(avgs)/len(avgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (CSE 258 only) Tasks (Rating prediction)\n",
    "\n",
    "Let’s start by building our training/validation sets much as we did for the first task. This time building a validation set is more straightforward: you can simply use part of the data for validation, and do not need to randomly sample non-read users/books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [line for line in readCSV(\"train_Interactions.csv.gz\")]\n",
    "Xy_train, Xy_valid = data[:190000], data[190000:]\n",
    "Xtrain, ytrain = [x[:2] for x in Xy_train], [int(x[-1]) for x in Xy_train]\n",
    "Xvalid, yvalid = [x[:2] for x in Xy_valid], [int(x[-1]) for x in Xy_valid]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 9\n",
    "Fit a predictor of the form\n",
    "\n",
    "$rating(user, item) = \\alpha + \\beta_{user} + \\beta_{item}$\n",
    "\n",
    "\n",
    "by fitting the mean and the two bias terms as described in the lecture notes. Use a regularization\n",
    "parameter of λ = 1. Report the MSE on the validation set (1 mark)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(predictions, labels):\n",
    "    differences = [(x-y)**2 for x,y in zip(predictions,labels)]\n",
    "    return sum(differences) / len(differences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewsPerUser = defaultdict(list)\n",
    "reviewsPerBook = defaultdict(list)\n",
    "\n",
    "for user, book, rating in Xy_train:\n",
    "    rating = int(rating)\n",
    "    reviewsPerUser[user].append((book,rating))\n",
    "    reviewsPerBook[book].append((user,rating))\n",
    "\n",
    "N = len(ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "usersPerBook = defaultdict(set)\n",
    "bookPerUser = defaultdict(set)\n",
    "for line in Xy_train:\n",
    "    userID, bookID, rating = line\n",
    "    usersPerBook[bookID].add(userID)\n",
    "    bookPerUser[userID].add(bookID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(user, book):\n",
    "    return alpha + userBiases[user] + bookBiases[book]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "lamb = 0.000001\n",
    "userBiases = defaultdict(float)\n",
    "bookBiases = defaultdict(float)\n",
    "for user in bookPerUser.keys():\n",
    "    userBiases[user] = random.random() - 0.5\n",
    "for book in usersPerBook.keys():\n",
    "    bookBiases[book] = random.random() - 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9220745696523946\n",
      "0.9088652390055492\n",
      "0.9083792457082627\n",
      "0.9082930255200622\n",
      "0.9082698300389569\n",
      "0.9082649437920396\n",
      "0.9082653299066792\n",
      "0.9082667995216558\n",
      "0.9082681434661425\n",
      "0.9082691228135656\n"
     ]
    }
   ],
   "source": [
    "# Alt 1\n",
    "def cost(lamb, userBiases, bookBiases, X, y):\n",
    "    predictions = [prediction(user, book) for user, book in X]\n",
    "    cost = MSE(predictions, y) # Error\n",
    "    \n",
    "    # And add regularization\n",
    "    for u in userBiases:\n",
    "        cost += lamb*userBiases[u]**2\n",
    "    for b in bookBiases:\n",
    "        cost += lamb*bookBiases[b]**2\n",
    "    return cost\n",
    "\n",
    "# Train\n",
    "for x in range(10):\n",
    "    \n",
    "    # Calculate alpha\n",
    "    temp = sum([int(rating) - (userBiases[user] + bookBiases[book]) for user, book, rating in Xy_train])\n",
    "    alpha = temp / N\n",
    "    \n",
    "    # Calculate Beta_user\n",
    "    for user in userBiases.keys():\n",
    "        books = bookPerUser[user]\n",
    "        book_reviews = dict(reviewsPerUser[user])\n",
    "        temp = sum([book_reviews[book] - (alpha + bookBiases[book]) for book in books])\n",
    "        temp /= lamb + len(books)\n",
    "        userBiases[user] = temp\n",
    "    \n",
    "        \n",
    "    # Calculate Beta_book\n",
    "    for book in bookBiases.keys():\n",
    "        users = usersPerBook[book]\n",
    "        users_review = dict(reviewsPerBook[book])\n",
    "        temp = sum([users_review[user] - (alpha + userBiases[user]) for user in users])\n",
    "        temp /= lamb + len(users)\n",
    "        bookBiases[book] = temp\n",
    "    print(cost(lamb, userBiases, bookBiases, Xtrain, ytrain))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JOn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate(lambda_value):\n",
    "    beta_users = defaultdict(float)\n",
    "    for user in bookPerUser.keys():\n",
    "        beta_users[user] = 0\n",
    "        \n",
    "    beta_items = defaultdict(float)\n",
    "    for book in usersPerBook.keys():\n",
    "        beta_items[book] = 0\n",
    "    \n",
    "    def alpha1():\n",
    "        a = 0\n",
    "        N = len(Xtrain)\n",
    "        for X, r in zip(Xtrain, ytrain):\n",
    "            user, book = X\n",
    "            r = int(r)\n",
    "            a += (r - (beta_users[user] + beta_items[book]))\n",
    "        return a/N\n",
    "        \n",
    "    def beta_user(a):\n",
    "        for X, r in zip(Xtrain, ytrain):\n",
    "            user, book = X\n",
    "            r = int(r)\n",
    "            N = lambda_value + len(reviewsPerUser[user])\n",
    "            beta_users[user] += (r - (a + beta_items[book])) / N\n",
    "        \n",
    "    \n",
    "    def beta_item(a):\n",
    "        for X, r in zip(Xtrain, ytrain):\n",
    "            user, book = X\n",
    "            r = int(r)\n",
    "            N = lambda_value + len(reviewsPerBook[book])\n",
    "            beta_items[book] += (r - (a + beta_users[user]))/ N\n",
    "    \n",
    "    last_alpha = 1\n",
    "    current_alpha = 0    \n",
    "    while abs(current_alpha - last_alpha)  > 0.001:\n",
    "        last_alpha = current_alpha\n",
    "        \n",
    "        current_alpha = alpha1()\n",
    "        print(current_alpha)\n",
    "        for user in bookPerUser.keys():\n",
    "            beta_users[user] = 0\n",
    "        beta_user(current_alpha)\n",
    "        \n",
    "        for book in usersPerBook.keys():\n",
    "            beta_items[book] = 0\n",
    "        beta_item(current_alpha)\n",
    "        \n",
    "    \n",
    "    return current_alpha, beta_users, beta_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.897121052631579\n",
      "3.890945336866959\n",
      "3.884670470570013\n",
      "3.8788226299683974\n",
      "3.8734246396728955\n",
      "3.8684450920264095\n",
      "3.8638508425080094\n",
      "3.859612133751425\n",
      "3.8557021709354355\n",
      "3.8520964528826807\n",
      "3.848772350821494\n",
      "3.8457088614250576\n",
      "3.8428864510646696\n",
      "3.8402869445492813\n",
      "3.8378934354206065\n",
      "3.8356902071204293\n",
      "3.8336626602356247\n",
      "3.8317972437534804\n",
      "3.830081389502148\n",
      "3.8285034494865458\n",
      "3.82705263604893\n",
      "3.8257189648549814\n",
      "3.824493200718295\n",
      "3.8233668062616704\n",
      "3.8223318933910373\n",
      "3.8213811775381554\n"
     ]
    }
   ],
   "source": [
    "alpha, userBiases, bookBiases = calculate(2.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALT 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 782,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1556.0249390559\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-782-3de1bbe6eaa1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mdalpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdUserBiases\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdBookBiases\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mderivative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlamb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserBiases\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbookBiases\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXy_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0malpha\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdalpha\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-782-3de1bbe6eaa1>\u001b[0m in \u001b[0;36mderivative\u001b[0;34m(lamb, alpha, userBiases, bookBiases, Xy_train)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# Book\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0musers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0musersPerBook\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbook\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0malpha\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0muserBiases\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muser2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbookBiases\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbook\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mrating\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0muser2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0musers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mtemp\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m \u001b[0mlamb\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbookBiases\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbook\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# Regularization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mdBookBiases\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbook\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemp\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mN\u001b[0m \u001b[0;31m# Maybe remove N\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-782-3de1bbe6eaa1>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# Book\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0musers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0musersPerBook\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbook\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0malpha\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0muserBiases\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muser2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbookBiases\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbook\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mrating\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0muser2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0musers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mtemp\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m \u001b[0mlamb\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbookBiases\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbook\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# Regularization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mdBookBiases\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbook\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemp\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mN\u001b[0m \u001b[0;31m# Maybe remove N\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ALT 2\n",
    "import random\n",
    "lamb = 1\n",
    "lr = 1\n",
    "alpha = ratingMean\n",
    "userBiases = defaultdict(float)\n",
    "bookBiases = defaultdict(float)\n",
    "for user in bookPerUser.keys():\n",
    "    userBiases[user] = random.random() - 0.5\n",
    "for book in usersPerBook.keys():\n",
    "    bookBiases[book] = random.random() - 0.5\n",
    "    \n",
    "def derivative(lamb, alpha, userBiases, bookBiases, Xy_train):\n",
    "    dalpha = 0\n",
    "    dUserBiases = defaultdict(float)\n",
    "    dBookBiases = defaultdict(float)\n",
    "    for user, book, rating in Xy_train:\n",
    "        rating = int(rating)\n",
    "        \n",
    "        # Alpha\n",
    "        dalpha += (2/N) * (alpha + userBiases[user] + bookBiases[book] - rating)\n",
    "        \n",
    "        # User\n",
    "        books = bookPerUser[user]\n",
    "        temp = 2*sum([alpha + userBiases[user] + bookBiases[book2] - rating for book2 in books])\n",
    "        temp += 2* lamb * userBiases[user] # Regularization\n",
    "        dUserBiases[user] = temp / N # Maybe remove N\n",
    "        \n",
    "        # Book\n",
    "        users = usersPerBook[book]\n",
    "        temp = 2*sum([alpha + userBiases[user2] + bookBiases[book] - rating for user2 in users])\n",
    "        temp += 2* lamb * bookBiases[book] # Regularization\n",
    "        dBookBiases[book] = temp / N # Maybe remove N\n",
    "        \n",
    "    return dalpha, dUserBiases,dBookBiases\n",
    "\n",
    "for x in range(10):\n",
    "    \n",
    "    dalpha, dUserBiases, dBookBiases = derivative(lamb, alpha, userBiases, bookBiases, Xy_train)\n",
    "    \n",
    "    alpha -= lr * dalpha\n",
    "    for user in userBiases.keys():\n",
    "        userBiases[user] -= lr * dUserBiases[user]\n",
    "    for book in bookBiases.keys():\n",
    "        bookBiases[book] -= lr * dBookBiases[book]\n",
    "    \n",
    "    print(cost(lamb, userBiases, bookBiases, Xtrain, ytrain))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 1.1084461391876665\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "for user, book in Xvalid:\n",
    "    predictions.append(prediction(user, book))\n",
    "\n",
    "mse = MSE(predictions, yvalid)\n",
    "print(\"MSE: {}\".format(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 10\n",
    "Report the user and book IDs that have the largest and smallest values of β (1 mark)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Largest user bias: ('u32162993', 1.5065993547347916) \n",
      " Smallest user bias: ('u47604248', -4.1307309523718345)\n",
      "\n",
      " Largest book bias: ('b19925500', 1.6016881017938551) \n",
      " Smallest book bias: ('b84091840', -2.1038598817102456)\n"
     ]
    }
   ],
   "source": [
    "# Code here\n",
    "userBiases_items = list(userBiases.items())\n",
    "userBiases_items.sort(key=lambda x: x[1], reverse=True)\n",
    "print(\"\\n Largest user bias: {} \\n Smallest user bias: {}\".format(userBiases_items[0], userBiases_items[-1]))\n",
    "\n",
    "bookBiases_items = list(bookBiases.items())\n",
    "bookBiases_items.sort(key=lambda x: x[1], reverse=True)\n",
    "print(\"\\n Largest book bias: {} \\n Smallest book bias: {}\".format(bookBiases_items[0], bookBiases_items[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 11\n",
    "Find a better value of λ using your validation set. Report the value you chose, its MSE, and upload your solution to Kaggle by running it on the test data (1 mark).\n",
    "# Kaggle Username: kristogj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "lamb = 0.00002\n",
    "x, f, d = scipy.optimize.fmin_l_bfgs_b(cost, [alpha] + [0.0]*(nUsers+nBooks),\n",
    "                             derivative, args = (ytrain, lamb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I found the right value for lamb by looping over different values of lamb and observing how the mse was chaning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 1.1742062925437837\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "for user, book in Xvalid:\n",
    "    predictions.append(prediction(user, book))\n",
    "\n",
    "mse = MSE(predictions, yvalid)\n",
    "print(\"MSE: {}\".format(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = open(\"predictions_Rating.txt\", 'w')\n",
    "for l in open(\"pairs_Rating.txt\"):\n",
    "    if l.startswith(\"userID\"):\n",
    "        #header\n",
    "        predictions.write(l)\n",
    "        continue\n",
    "    user, book = l.strip().split('-')\n",
    "    pred = str(prediction(user, book))\n",
    "    predictions.write(user + '-' + book + ',' + pred + '\\n')\n",
    "predictions.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
