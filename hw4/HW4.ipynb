{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSE 258, Fall 2019: Homework 4\n",
    "Data: : http://cseweb.ucsd.edu/classes/fa19/cse258-a/files/assignment1.tar.gz\n",
    "\n",
    "Using the code provided on the webpage, read the first 10,000 reviews from the corpus, and read the reviews without capitalization or punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import gzip\n",
    "import string\n",
    "from collections import defaultdict\n",
    "from sklearn import linear_model\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "def readGz(path):\n",
    "    for l in gzip.open(path, 'rt'):\n",
    "        yield eval(l)\n",
    "\n",
    "punctuation = set(string.punctuation)\n",
    "data = []\n",
    "for line in readGz(\"train_Category.json.gz\"):\n",
    "    line[\"review_text\"] = ''.join([c for c in line['review_text'].lower() if not c in punctuation])\n",
    "    data.append(line)\n",
    "data = data[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_votes': 0,\n",
       " 'review_id': 'r99763621',\n",
       " 'user_id': 'u17334941',\n",
       " 'review_text': 'genuinely enthralling if collins or bernard did invent this out of whole cloth they deserve a medal for imagination lets leave the veracity aside for a moment  always a touchy subject when it comes to real life stories of the occult  and talk about the contents \\n the black alchemist covers a period of two years in which collins a magician and bernard a psychic undertook a series of psychic quests that put them in opposition with the titular black alchemist as entertainment goes the combination of harrowing discoveries ancient lore and going down the pub for a cigarette and a guinness trying to make sense of it all while a hen party screams at each other is a winner it is simultaneously down to earth and out of this world \\n it reads fast both because of the curiousity and because collins has a very clear writing style sometimes its a little clunky or over repetitive and theres a few meetings that get underreported but i am very much quibbling here mostly important he captures his own and bernards sense of wonder awe and occasionally revulsion enough that i shared them',\n",
       " 'rating': 5,\n",
       " 'genreID': 2,\n",
       " 'genre': 'fantasy_paranormal'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "How many unique bigrams are there amongst the reviews? List the 5 most-frequently-occurring bigrams\n",
    "along with their number of occurrences in the corpus (1 mark)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = defaultdict(int)\n",
    "for d in data:\n",
    "    review = d[\"review_text\"].split()\n",
    "    for x in range(1, len(review)):\n",
    "        a,b = review[x-1].strip(), review[x].strip()\n",
    "        bigram = \"{} {}\".format(a,b)\n",
    "        bigrams[bigram] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 most-frequently-occuring bigrams and their number of coccurences:\n",
      "\"of the\": 7927\n",
      "\"this book\": 5850\n",
      "\"in the\": 5627\n",
      "\"and the\": 3189\n",
      "\"is a\": 3183\n"
     ]
    }
   ],
   "source": [
    "bigram_items = list(bigrams.items())\n",
    "bigram_items.sort(key=lambda item:item[1], reverse=True)\n",
    "print(\"5 most-frequently-occuring bigrams and their number of coccurences:\")\n",
    "for bigram, count in bigram_items[:5]:\n",
    "    print('\"{}\": {}'.format(bigram, count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "The code provided performs least squares using the 1000 most common unigrams. Adapt it to use\n",
    "the 1000 most common bigrams and report the MSE obtained using the new predictor (use bigrams\n",
    "only, i.e., not unigrams+bigrams) (1 mark). Note that the code performs regularized regression with a\n",
    "regularization parameter of 1.0. The prediction target should be the ‘rating’ field in each review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [x[0] for x in bigram_items[:1000]]\n",
    "wordId = dict(zip(words, range(len(words))))\n",
    "wordSet = set(words)\n",
    "\n",
    "def feature(datum):\n",
    "    feat = [0]*len(words)\n",
    "    review = datum[\"review_text\"].split()\n",
    "    for x in range(1, len(review)):\n",
    "        a,b = review[x-1].strip(), review[x].strip()\n",
    "        bigram = \"{} {}\".format(a,b)\n",
    "        if bigram in wordSet:\n",
    "            feat[wordId[bigram]] += 1\n",
    "    feat.append(1) #offset\n",
    "    return feat\n",
    "\n",
    "X = [feature(d) for d in data]\n",
    "y = [d['rating'] for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 1.0178487590567005\n"
     ]
    }
   ],
   "source": [
    "def MSE(predictions, labels):\n",
    "    differences = [(x-y)**2 for x,y in zip(predictions,labels)]\n",
    "    return sum(differences) / len(differences)\n",
    "    \n",
    "# Regularized regression\n",
    "clf = linear_model.Ridge(1.0, fit_intercept=False) # MSE + 1.0 l2\n",
    "clf.fit(X, y)\n",
    "theta = clf.coef_\n",
    "predictions = clf.predict(X)\n",
    "\n",
    "# Evaluate\n",
    "mse = MSE(predictions, y)\n",
    "print(\"MSE: {}\".format(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "Repeat the above experiment using unigrams and bigrams, still considering the 1000 most common.\n",
    "That is, your model will still use 1000 features (plus an offset), but those 1000 features will be some\n",
    "combination of unigrams and bigrams. Report the MSE obtained using the new predictor (1 mark)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"the\": 73431\n",
      "\"and\": 44301\n",
      "\"a\": 39577\n",
      "\"to\": 36821\n",
      "\"i\": 36581\n"
     ]
    }
   ],
   "source": [
    "unigrams = defaultdict(int)\n",
    "for d in data:\n",
    "    review = d[\"review_text\"].split()\n",
    "    for word in review:\n",
    "        unigrams[word] += 1\n",
    "unigram_items = list(unigrams.items())\n",
    "unigram_items.sort(key=lambda item: item[1], reverse=True)\n",
    "for unigram, count in unigram_items[:5]:\n",
    "    print('\"{}\": {}'.format(unigram, count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge and sort lists of uni- and bigrams\n",
    "uni_and_bigrams = unigram_items + bigram_items\n",
    "uni_and_bigrams.sort(key=lambda item:item[1], reverse=True)\n",
    "\n",
    "# Find the 1000 most common of these\n",
    "words = [x[0] for x in uni_and_bigrams[:1000]]\n",
    "wordId = dict(zip(words, range(len(words))))\n",
    "wordSet = set(words)\n",
    "\n",
    "def feature(datum):\n",
    "    feat = [0]*len(words)\n",
    "    review = datum[\"review_text\"].split()\n",
    "    \n",
    "    # Check unigrams\n",
    "    for word in review:\n",
    "        if word in wordSet:\n",
    "            feat[wordId[word]] += 1\n",
    "            \n",
    "    # Check bigrams\n",
    "    for x in range(1, len(review)):\n",
    "        a,b = review[x-1].strip(), review[x].strip()\n",
    "        bigram = \"{} {}\".format(a,b)\n",
    "        if bigram in wordSet:\n",
    "            feat[wordId[bigram]] += 1\n",
    "    \n",
    "    feat.append(1) #offset\n",
    "    return feat\n",
    "\n",
    "X = [feature(d) for d in data]\n",
    "y = [d['rating'] for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.9683729530414926\n"
     ]
    }
   ],
   "source": [
    "# Regularized regression\n",
    "clf = linear_model.Ridge(1.0, fit_intercept=False) # MSE + 1.0 l2\n",
    "clf.fit(X, y)\n",
    "theta = clf.coef_\n",
    "predictions = clf.predict(X)\n",
    "\n",
    "# Evaluate\n",
    "mse = MSE(predictions, y)\n",
    "print(\"MSE: {}\".format(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4\n",
    " What is the inverse document frequency of the words ‘stories’, ‘magician’, ‘psychic’, ‘writing’, and ‘wonder’? What are their tf-idf scores in the first review (using log base 10) (1 mark)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the document frequencies of each unigram\n",
    "dfs = defaultdict(int)\n",
    "check_words = ['stories', 'magician', 'psychic', 'writing', 'wonder']\n",
    "for d in data:\n",
    "    review = d[\"review_text\"].split()\n",
    "    for word in check_words:\n",
    "        if word in review:\n",
    "            dfs[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDF stories: 1.1174754620451195\n",
      "IDF magician: 2.657577319177794\n",
      "IDF psychic: 2.6020599913279625\n",
      "IDF writing: 0.9978339382434923\n",
      "IDF wonder: 1.7670038896078462\n"
     ]
    }
   ],
   "source": [
    "N = len(data) # Total number of documents\n",
    "idfs = dict()\n",
    "# Calculate inverse document frequencies\n",
    "for word in check_words:\n",
    "    df = dfs[word]\n",
    "    idfs[word] = np.log10(N/df)\n",
    "    print(\"IDF {}: {}\".format(word, idfs[word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF stories: 1.1174754620451195\n",
      "TFIDF magician: 2.657577319177794\n",
      "TFIDF psychic: 5.204119982655925\n",
      "TFIDF writing: 0.9978339382434923\n",
      "TFIDF wonder: 1.7670038896078462\n"
     ]
    }
   ],
   "source": [
    "# Calculate TF-IDF for first document\n",
    "tfidfs = dict()\n",
    "first_review = data[0][\"review_text\"].split()\n",
    "for word in check_words:\n",
    "    tf = first_review.count(word)\n",
    "    idf = idfs[word]\n",
    "    tfidfs[word] = tf * idf\n",
    "    print(\"TFIDF {}: {}\".format(word, tfidfs[word]))\n",
    "    \n",
    "                           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5\n",
    "Adapt your unigram model to use the tfidf scores of words, rather than a bag-of-words representation.\n",
    "That is, rather than your features containing the word counts for the 1000 most common unigrams, it\n",
    "should contain tfidf scores for the 1000 most common unigrams. Report the MSE of this new model (1\n",
    "mark)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6\n",
    "Which other review has the highest cosine similarity compared to the first review (provide the review id,\n",
    "or the text of the review) (1 mark)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7\n",
    "Implement a validation pipeline for this same data, by randomly shuffling the data, using 10,000 reviews\n",
    "for training, another 10,000 for validation, and another 10,000 for testing.1 Consider regularization\n",
    "parameters in the range {0.01, 0.1, 1, 10, 100}, and report MSEs on the test set for the model that performs\n",
    "best on the validation set. Using this pipeline, compare the following alternatives in terms of their\n",
    "performance:\n",
    "* Unigrams vs. bigrams\n",
    "* Removing punctuation vs. preserving it. The model that presevers punctuation sshould treat punctiation charactes as seperate works, e.g. \"Amazing!\" would become [\"amazing\", \"!\"]\n",
    "* tfidf scores vs. word counts   \n",
    "In total you should compare 2 × 2 × 2 = 8 models, and produce a table comparing their performance (2\n",
    "marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
